{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YadKFDgcs2Hh"
      },
      "source": [
        "## PrÃ©-processamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQz93nKHs2Hl"
      },
      "source": [
        "base: https://archive-beta.ics.uci.edu/dataset/174/parkinsons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Q_YaAG3vs2Hm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pandas import DataFrame\n",
        "\n",
        "rs = 1              # Random State\n",
        "folds = 10          # Quantity of folds\n",
        "test_split = 0.25   # Validation\n",
        "exec_time = 5       # Time of each AutoML (minutes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VngT1pxRs2Hn"
      },
      "outputs": [],
      "source": [
        "park_data = pd.read_csv('../bases/test_bases/parkinsons.data', header=None)\n",
        "\n",
        "park_data.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYoRG8mps2Ho"
      },
      "outputs": [],
      "source": [
        "feature_names = list(park_data.iloc[0])\n",
        "\n",
        "df = park_data.drop(park_data.index[0])\n",
        "df = df.drop(0, axis=1)\n",
        "df = df.astype(float)\n",
        "df = df.rename(columns = pd.Series(feature_names))\n",
        "\n",
        "feature_names.remove('name')\n",
        "\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZE4CWykcs2Hp"
      },
      "outputs": [],
      "source": [
        "sns.countplot(x=\"status\", data= df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJ-ubgzJs2Hp"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "X = df.drop(columns=\"status\", axis=1).values\n",
        "y = df[\"status\"].values\n",
        "feature_names.remove(\"status\")\n",
        "\n",
        "std = StandardScaler()\n",
        "X = std.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size= test_split, random_state=rs)\n",
        "\n",
        "kfold = StratifiedKFold(n_splits= folds, random_state= rs, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdinbJU-s2Hq"
      },
      "outputs": [],
      "source": [
        "from imblearn.under_sampling import TomekLinks\n",
        "from imblearn.over_sampling import SMOTE\n",
        "# Oversampling unbanlanced data\n",
        "sm = SMOTE(random_state=rs)\n",
        "X_over, y_over = sm.fit_resample(X_train , y_train)\n",
        "\n",
        "# Undersampling unbalanced data\n",
        "tl = TomekLinks(sampling_strategy=\"auto\")\n",
        "X_under, y_under = tl.fit_resample(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jxl_wOBAs2Hr"
      },
      "outputs": [],
      "source": [
        "print(np.unique(y_over, return_counts=True),'\\n',\n",
        "      np.unique(y_under, return_counts = True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Cfnkr4us2Hr"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "def cross_val(classifier, X_train, y_train) -> np.array:\n",
        "\n",
        "    model_results = []\n",
        "\n",
        "    for train_index, test_index in kfold.split(X_train, y_train):\n",
        "\n",
        "        X_fold_train, y_fold_train = X_train[train_index], y_train[train_index]\n",
        "        X_fold_test, y_fold_test = X_train[test_index], y_train[test_index]\n",
        "\n",
        "        classifier.fit(X_fold_train, y_fold_train)\n",
        "        y_pred = classifier.predict(X_fold_test)\n",
        "        model_results.append(accuracy_score(y_fold_test, y_pred))\n",
        "\n",
        "    return np.array(model_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xy-eWWsIs2Hs"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "\n",
        "def compute_shap(model, feature_names, X_train, explainer_ = 'kernel', num_samples = 50) -> DataFrame:\n",
        "\n",
        "    shap.initjs()\n",
        "\n",
        "    #Defining and executing explainer\n",
        "    sample = shap.sample(X_train, num_samples, random_state=rs)\n",
        "    explainer = shap.TreeExplainer(model) if explainer_ == 'tree' else shap.KernelExplainer(model.predict, sample)\n",
        "    shap_values = explainer.shap_values(sample)\n",
        "\n",
        "    #Calculate shap_values impact of each feature\n",
        "    shap_sum = np.abs(shap_values).mean(axis=0)\n",
        "\n",
        "    #Make a dataframe of each feature and shap_value\n",
        "    importance_df = pd.DataFrame([feature_names, shap_sum.tolist()]).T\n",
        "    importance_df.columns = ['feature', 'importance']\n",
        "    importance_df = importance_df.sort_values('importance', ascending=False)\n",
        "\n",
        "    features = importance_df['feature'].index.to_list()\n",
        "\n",
        "    shap.summary_plot(shap_values, sample, feature_names=feature_names)\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLxDvjYqs2Hs"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import clone\n",
        "\n",
        "def feature_selection(model_, ordered_features, X_train = X_train, X_test = X_test, y_train = y_train, y_test = y_test, n_min = -1, threshold=0.03):\n",
        "\n",
        "    features_list = ordered_features[:n_min].copy() if n_min >= 0 else ordered_features.copy()\n",
        "\n",
        "    best_features = None\n",
        "    best_acc = 0\n",
        "    current = features_list.copy()\n",
        "\n",
        "    params = model_.get_params()\n",
        "\n",
        "    for feature in features_list[1:][::-1]:\n",
        "        current.remove(feature)\n",
        "\n",
        "        X_train_shap = X_train[:, current]\n",
        "        X_test_shap = X_test[:, current]\n",
        "\n",
        "        if 'max_features' in params:\n",
        "            if len(current) == params['max_features']:\n",
        "                print('Limite do modelo.')\n",
        "                break\n",
        "\n",
        "        if 'warm_start' in params:\n",
        "            model = clone(model_)\n",
        "        else:\n",
        "            model = model_\n",
        "\n",
        "        model.fit(X_train_shap, y_train)\n",
        "        y_pred = model.predict(X_test_shap)\n",
        "        test_acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        if test_acc > best_acc-best_acc*threshold:\n",
        "            best_acc = test_acc\n",
        "            best_features = current.copy()\n",
        "            print(f'Best accuracy: {best_acc} with {len(best_features)} features')\n",
        "\n",
        "    print(f'Results: Best accuracy: {best_acc} with {best_features}')\n",
        "\n",
        "    return best_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnGBeTu_s2Ht"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "\n",
        "def run_test(models: list, X_train_ = X_train, y_train_= y_train ,X_test_ = X_test, y_test_ = y_test, cm = False, new_features = None):\n",
        "\n",
        "    if new_features != None:\n",
        "        print(\"New features: \", new_features, end=\"\\n\\n\")\n",
        "        X_train_ = X_train_[:,new_features]\n",
        "        X_test_ = X_test_[:,new_features]\n",
        "\n",
        "    for model_ in models:\n",
        "\n",
        "        model = clone(model_)\n",
        "        model.fit(X_train_, y_train_)\n",
        "        y_pred = model.predict(X_test_)\n",
        "        test_acc = accuracy_score(y_test_, y_pred)\n",
        "\n",
        "        print(model)\n",
        "        print(\"Accuracy: \", test_acc, end=\"\\n\\n\")\n",
        "\n",
        "        if cm:\n",
        "            matrix = ConfusionMatrixDisplay(confusion_matrix(y_test_, y_pred))\n",
        "            matrix.plot()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fs_XAPys2Ht"
      },
      "source": [
        "# Teste com SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ll6tOxYHs2Ht"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "model = SVC()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRq_6vSos2Ht"
      },
      "outputs": [],
      "source": [
        "run_test([model], X_test, y_test)\n",
        "run_test([model], X_over, y_over)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jK8yIdUXs2Ht"
      },
      "source": [
        "## Tunning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQno8788s2Hu"
      },
      "outputs": [],
      "source": [
        "#Tunning\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'kernel': ['linear', 'rbf', 'poly'],\n",
        "    'gamma': ['scale', 'auto', 0.1, 1],\n",
        "    'coef0': [0.0, 0.5, 1.0],\n",
        "    'shrinking': [True, False],\n",
        "    'probability': [True],\n",
        "    'random_state': [rs]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8nOrMK5s2Hu"
      },
      "outputs": [],
      "source": [
        "print(\"Grid Search...\")\n",
        "grid_search = GridSearchCV(model, param_grid, n_jobs=4, cv= kfold)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Grid Search (over)...\")\n",
        "grid_search_over = GridSearchCV(model, param_grid, n_jobs=4, cv= kfold)\n",
        "grid_search_over.fit(X_over, y_over)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qe2eICgps2Hu"
      },
      "outputs": [],
      "source": [
        "print(\"Melhores parametros: \", grid_search.best_params_)\n",
        "print(\"Melhores parametros: \", grid_search_over.best_params_)\n",
        "\n",
        "model_gs = grid_search.best_estimator_\n",
        "model_gs_over = grid_search_over.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efS4gl7Ds2Hu"
      },
      "outputs": [],
      "source": [
        "run_test([model, model_gs])\n",
        "run_test([model, model_gs_over], X_over, y_over)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zW3Tn5GYs2Hv"
      },
      "source": [
        "## Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8S-7m4YUs2Hv"
      },
      "outputs": [],
      "source": [
        "shap_svm = compute_shap(model_gs, feature_names, X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJX-e2vfs2Hv"
      },
      "outputs": [],
      "source": [
        "svm_features = feature_selection(model_gs, shap_svm, threshold=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwITtPdss2Hv"
      },
      "outputs": [],
      "source": [
        "run_test([model_gs], cm = True, new_features=svm_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OexSJ7As2Hv"
      },
      "source": [
        "# Auto Sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0Sb3APws2Hw"
      },
      "outputs": [],
      "source": [
        "from autosklearn.classification import AutoSklearnClassifier\n",
        "from autosklearn.metrics import balanced_accuracy, precision, recall, f1\n",
        "\n",
        "minutos = 1\n",
        "\n",
        "autosk = AutoSklearnClassifier(\n",
        "    include = {\n",
        "        \"classifier\": [\"random_forest\", \"decision_tree\", \"extra_trees\",\n",
        "                        'liblinear_svc', 'libsvm_svc','k_nearest_neighbors'],\n",
        "        \"feature_preprocessor\":[\"no_preprocessing\"]\n",
        "    },\n",
        "    time_left_for_this_task= minutos*60,\n",
        "    per_run_time_limit= 30,\n",
        "    scoring_functions= [balanced_accuracy, precision, recall, f1],\n",
        "    ensemble_class= 'none',\n",
        "    #ensemble_nbest = 25,\n",
        "    n_jobs = 4\n",
        ")\n",
        "\n",
        "autosk.fit(X_train, y_train, dataset_name= 'Parkinson Disease Prediction')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7SgoekKs2Hw"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import clone\n",
        "\n",
        "models = autosk.show_models()\n",
        "best_model = list(models.keys())[0]\n",
        "\n",
        "sklearn_classifier = clone(models[best_model][\"sklearn_classifier\"])\n",
        "sklearn_classifier.fit(X_train, y_train)\n",
        "\n",
        "print(sklearn_classifier, '\\n', autosk.sprint_statistics())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuybrBEfs2Hw"
      },
      "outputs": [],
      "source": [
        "params = sklearn_classifier.get_params()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA5G5UtHs2Hw"
      },
      "source": [
        "## Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWkJDUs0s2Hw"
      },
      "outputs": [],
      "source": [
        "shap_autosk = compute_shap(sklearn_classifier, feature_names, X_train, num_samples=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hLKIYcLs2Hx"
      },
      "outputs": [],
      "source": [
        "autosk_features = feature_selection(sklearn_classifier, shap_autosk, threshold=0.03)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWeJbsVVs2Hx"
      },
      "outputs": [],
      "source": [
        "run_test([sklearn_classifier], cm = True, new_features= autosk_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWzjquLBs2Hx"
      },
      "source": [
        "# Fla ML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbpvyG_Ks2Hx"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedGroupKFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvAeQqGfs2Hy"
      },
      "outputs": [],
      "source": [
        "from flaml import AutoML\n",
        "\n",
        "minutos = 1\n",
        "\n",
        "fla_automl = AutoML()\n",
        "\n",
        "fla_automl_settings = {\n",
        "    \"time_budget\": minutos*60,\n",
        "    \"metric\": 'accuracy',\n",
        "    \"n_jobs\": -1,\n",
        "    \"ensemble\": False,\n",
        "    \"verbose\": 0,\n",
        "    \"n_splits\": 5\n",
        "}\n",
        "\n",
        "fla_automl.fit(X_train, y_train, task= \"classification\", **fla_automl_settings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBex8D_is2Hy"
      },
      "outputs": [],
      "source": [
        "run_test([flaml_classifier], cm = True)\n",
        "run_test([flaml_classifier], X_over, y_over, cm = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-S8Bw5Ps2Hz"
      },
      "source": [
        "## Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIDh5Xf0s2Hz"
      },
      "outputs": [],
      "source": [
        "shap_flaml = compute_shap(flaml_classifier, feature_names, X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTV4BZY0s2H0"
      },
      "outputs": [],
      "source": [
        "flaml_features = feature_selection(flaml_classifier, shap_flaml, threshold=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBLuSvqEs2H0"
      },
      "outputs": [],
      "source": [
        "run_test([flaml_classifier], cm = True, new_features= flaml_features)\n",
        "run_test([flaml_classifier], X_over, y_over, cm = True, new_features= flaml_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whLek1Xzs2H0"
      },
      "source": [
        "# AnÃ¡lise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0ULMRiZs2H0"
      },
      "outputs": [],
      "source": [
        "from matplotlib_venn import venn3_unweighted\n",
        "\n",
        "set1 = set(svm_features)\n",
        "set2 = set(autosk_features)\n",
        "set3 = set(flaml_features)\n",
        "\n",
        "subsets = (set1 - set2 - set3,\n",
        "           set2 - set1 - set3,\n",
        "           set1 & set2 - set3,\n",
        "           set3 - set1 - set2,\n",
        "           set1 & set3 - set2,\n",
        "           set2 & set3 - set1,\n",
        "           set1 & set2 & set3)\n",
        "\n",
        "venn3_unweighted(subsets, set_labels=('SVM', 'AutoSklearn', 'FlaML'))\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kw-98gtys2H1"
      },
      "outputs": [],
      "source": [
        "print(autosk_features)\n",
        "print(svm_features)\n",
        "print(flaml_features)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mlproject",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
